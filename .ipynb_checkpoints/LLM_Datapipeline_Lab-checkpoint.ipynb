{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae182f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers, AutoTokenizer, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190f9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e6a57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in dataset: 36718\n"
     ]
    }
   ],
   "source": [
    "# 1. Load a text dataset (we use a small example dataset for demonstration)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")  # raw text WikiText-2\n",
    "print(f\"Number of lines in dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb52a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: facebook/opt-125m\n",
      "Vocab size: 50265\n",
      "Pad token: <pad> eos token: </s>\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded:\", model_name)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Pad token:\", tokenizer.pad_token, \"eos token:\", tokenizer.eos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd45b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 2428602; Full 128-token blocks: 18973\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# 3. Tokenize the dataset efficiently \n",
    "# Approach:\n",
    "#  - take the list of raw text strings\n",
    "#  - run tokenizer in Python in batches (tokenizer(texts, add_special_tokens=False))\n",
    "#  - concatenate all token ids into a single list of ids\n",
    "#  - split into non-overlapping blocks of block_size tokens\n",
    "#  - create a PyTorch Dataset that yields these blocks\n",
    "\n",
    "def batch_tokenize_texts(texts: List[str], tokenizer, batch_size: int = 512):\n",
    "    \"\"\"\n",
    "    Tokenize a list of strings in batches and return a flat list of token ids lists.\n",
    "    We use add_special_tokens=False so block splitting is straightforward.\n",
    "    \"\"\"\n",
    "    all_input_ids = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        enc = tokenizer(batch, add_special_tokens=False)\n",
    "        all_input_ids.extend(enc[\"input_ids\"])\n",
    "    return all_input_ids\n",
    "\n",
    "# Parameters\n",
    "block_size = 128\n",
    "batch_tokenize_batch_size = 512\n",
    "\n",
    "# Extract raw texts\n",
    "text_column = \"text\" if \"text\" in dataset.column_names else dataset.column_names[0]\n",
    "raw_texts = [x[text_column] for x in dataset]\n",
    "\n",
    "# Tokenize all lines in batches\n",
    "tokenized_lines = batch_tokenize_texts(raw_texts, tokenizer, batch_size=batch_tokenize_batch_size)\n",
    "\n",
    "# Concatenate token ids + EOS\n",
    "all_ids = []\n",
    "for ids in tokenized_lines:\n",
    "    all_ids.extend(ids + [tokenizer.eos_token_id])\n",
    "\n",
    "# Split into blocks\n",
    "num_full_blocks = len(all_ids) // block_size\n",
    "examples = []\n",
    "for i in range(num_full_blocks):\n",
    "    start = i * block_size\n",
    "    block_ids = all_ids[start : start + block_size]\n",
    "    examples.append(block_ids)\n",
    "\n",
    "print(f\"Total tokens: {len(all_ids)}; Full {block_size}-token blocks: {len(examples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe849645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader ready. Number of batches per epoch (approx): 2372\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BlockDataset(Dataset):\n",
    "    def __init__(self, blocks):\n",
    "        # blocks: list of list[int]\n",
    "        self.blocks = blocks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = torch.tensor(self.blocks[idx], dtype=torch.long)\n",
    "        return {\"input_ids\": ids, \"labels\": ids.clone()}\n",
    "\n",
    "lm_ds = BlockDataset(examples)\n",
    "\n",
    "# 5. Create a DataLoader for the tokenized, grouped dataset\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(lm_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "print(\"DataLoader ready. Number of batches per epoch (approx):\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a5f9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create a DataLoader for the tokenized, grouped dataset\n",
    "def collate_fn(batch):\n",
    "    # Since our sequences are fixed length after grouping, we might just stack them.\n",
    "    # If they weren't fixed, we could use tokenizer.pad to pad to max length in batch.\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in batch], dtype=torch.long)\n",
    "    # For language modeling, labels are the input_ids shifted by one, but \n",
    "    # Transformers' CausalLM models usually handle that internally if we provide labels = input_ids.\n",
    "    return {\"input_ids\": input_ids, \"labels\": input_ids.clone()}\n",
    "\n",
    "train_loader = DataLoader(lm_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edc38fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input_ids shape: torch.Size([8, 128])\n",
      "Batch labels shape: torch.Size([8, 128])\n",
      "First example token ids (first 20): [12096, 7403, 1746, 1325, 1131, 1416, 16392, 11, 5, 315, 532, 479, 252, 4596, 11, 59, 155, 787, 6, 1039]\n",
      "Decoded (first 200 chars):  000 burn injuries receive medical treatment yearly in the United States . They resulted in about 3 @,@ 300 deaths in 2008 . Most burns ( 70 % ) and deaths from burns occur in males . The highest inci\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Safe collate function that works whether __getitem__ returns lists or tensors\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of dicts like {\"input_ids\": <list or tensor>, \"labels\": <list or tensor>}\n",
    "    # convert each field to a tensor if needed, then stack\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    for item in batch:\n",
    "        inp = item[\"input_ids\"]\n",
    "        lab = item[\"labels\"]\n",
    "        if not isinstance(inp, torch.Tensor):\n",
    "            inp = torch.tensor(inp, dtype=torch.long)\n",
    "        if not isinstance(lab, torch.Tensor):\n",
    "            lab = torch.tensor(lab, dtype=torch.long)\n",
    "        input_ids_list.append(inp)\n",
    "        labels_list.append(lab)\n",
    "    input_ids = torch.stack(input_ids_list, dim=0)\n",
    "    labels = torch.stack(labels_list, dim=0)\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "# Recreate the DataLoader (adjust batch_size if you want)\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(lm_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Sanity-check: iterate one batch\n",
    "for batch in train_loader:\n",
    "    print(\"Batch input_ids shape:\", batch[\"input_ids\"].shape)   # (batch_size, block_size)\n",
    "    print(\"Batch labels shape:\", batch[\"labels\"].shape)\n",
    "    print(\"First example token ids (first 20):\", batch[\"input_ids\"][0][:20].tolist())\n",
    "    print(\"Decoded (first 200 chars):\", tokenizer.decode(batch[\"input_ids\"][0], skip_special_tokens=True)[:200])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e3492-ab44-444c-af6f-2ce548d3b0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
